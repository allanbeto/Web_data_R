knitr::opts_chunk$set(echo = TRUE)
packrat::snapshot(prompt = FALSE)
knitr::opts_chunk$set(echo = TRUE)
install.packages("httr")
install.packages("rvest")
install.packages("xml2")
packrat::snapshot()
packrat::snapshot()
packrat::snapshot()
remove.packages("pageviews", lib="")
install.packages("pageviews")
packrat::snapshot()
knitr::opts_chunk$set(echo = TRUE)
install.packages("rvest")
knitr::opts_chunk$set(echo = TRUE)
install.packages("jsonlite")
install.packages("xml2")
# install.packages("httr")
install.packages("rvest")
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"
# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)
# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)
# Examine the objects with head()
head(csv_data)
head(tsv_data)
# Download the file with download.file()
download.file(url = csv_url, destfile = "feed_data.csv")
# Read it in with read.csv()
csv_data <- read.csv("feed_data.csv")
# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight * csv_data$weight
# Save it to disk with saveRDS()
saveRDS(csv_data, file = "modified_feed_data.RDS")
# Read it back in with readRDS()
modified_feed_data <- readRDS("modified_feed_data.RDS")
# Examine modified_feed_data
str(modified_feed_data)
# Load pageviews
library("pageviews")
# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", "Hadley Wickham")
# Examine the resulting object
str(hadley_pageviews)
# Load birdnik
library("birdnik")
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
install.packages("birdnik")
packrat::snapshot()
packrat::snapshot()
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# install.packages("birdnik")
# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"
# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)
# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)
# Examine the objects with head()
head(csv_data)
head(tsv_data)
# Download the file with download.file()
download.file(url = csv_url, destfile = "feed_data.csv")
# Read it in with read.csv()
csv_data <- read.csv("feed_data.csv")
# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight * csv_data$weight
# Save it to disk with saveRDS()
saveRDS(csv_data, file = "modified_feed_data.RDS")
# Read it back in with readRDS()
modified_feed_data <- readRDS("modified_feed_data.RDS")
# Examine modified_feed_data
str(modified_feed_data)
# Load pageviews
library("pageviews")
# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", "Hadley Wickham")
# Examine the resulting object
str(hadley_pageviews)
# Load birdnik
library("birdnik")
# Get the word frequency for "vector", using api_key to access it
vector_frequency <- word_frequency(api_key, "vector")
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# install.packages("birdnik")
library("httr")
library("rvest")
library("jsonlite")
library("xml2")
library("birdnik")
# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"
# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)
# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)
# Examine the objects with head()
head(csv_data)
head(tsv_data)
# Download the file with download.file()
download.file(url = csv_url, destfile = "feed_data.csv")
# Read it in with read.csv()
csv_data <- read.csv("feed_data.csv")
# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight * csv_data$weight
# Save it to disk with saveRDS()
saveRDS(csv_data, file = "modified_feed_data.RDS")
# Read it back in with readRDS()
modified_feed_data <- readRDS("modified_feed_data.RDS")
# Examine modified_feed_data
str(modified_feed_data)
# Load pageviews
library("pageviews")
# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", "Hadley Wickham")
# Examine the resulting object
str(hadley_pageviews)
# Load birdnik
library("birdnik")
api_key <- "d8ed66f01da01b0c6a0070d7c1503801993a39c126fbc3382"
# Get the word frequency for "vector", using api_key to access it
vector_frequency <- word_frequency(api_key, "vector")
vector_frequency
# Load the httr package
library("httr")
# Make a GET request to http://httpbin.org/get
get_result <- GET("http://httpbin.org/get")
# Print it to inspect it
get_result
# Load the httr package
library("httr")
# Make a POST request to http://httpbin.org/post with the body "this is a test"
post_result <- POST("http://httpbin.org/post", body = "this is a test")
# Print it to inspect it
post_result
# Make a GET request to url and save the results
pageview_response <- GET(url)
url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102"
# Make a GET request to url and save the results
pageview_response <- GET(url)
# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)
# Examine the results with str()
str(pageview_data)
fake_url <- "http://google.com/fakepagethatdoesnotexist"
# Make the GET request
request_result <- GET(fake_url)
# Check request_result
if(http_error(request_result)){
TRUE
warning("The request failed")
} else {
content(request_result)
}
# Construct a directory-based API URL to `http://swapi.co/api`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", 1, sep = "/")
# Make a GET call with it
result <- GET(directory_url)
# Create list with nationality and country elements
query_params <- list(nationality = "americans",
country = "antigua")
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET("https://httpbin.org/get", query = query_params)
# Print parameter_response
parameter_response
# Do not change the url
url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100"
# Add the email address and the test sentence inside user_agent()
server_response <- GET(url, user_agent("my@email.address this is a test"))
# Construct a vector of 2 URLs
urls <- c("http://fakeurl.com/api/1.0/","http://fakeurl.com/api/2.0/")
for(url in urls){
# Send a GET request to url
result <- GET(url)
# Delay for 5 seconds between requests
Sys.sleep(5)
}
get_pageviews <- function(article_title){
url <- paste0("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents",
article_title,
"daily/2015100100/2015103100", sep = "/")
response <- GET(url, user_agent("my@email.com this is a test"))
if(http_error(response)){
stop("the request failed")
} else {
result <- content(response)
return(result)
}
}
rev_history <- function(title, format = "json"){
if (title != "Hadley Wickham") {
stop('rev_history() only works for `title = "Hadley Wickham"`')
}
if (format == "json"){
resp <- readRDS("had_rev_json.rds")
} else if (format == "xml"){
resp <- readRDS("had_rev_xml.rds")
} else {
stop('Invalid format supplied, try "json" or "xml"')
}
resp
}
# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")
download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_json.rds",  "had_rev_json.rds")
download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_xml.rds", "had_rev_xml.rds")
rev_history <- function(title, format = "json"){
if (title != "Hadley Wickham") {
stop('rev_history() only works for `title = "Hadley Wickham"`')
}
if (format == "json"){
resp <- readRDS("had_rev_json.rds")
} else if (format == "xml"){
resp <- readRDS("had_rev_xml.rds")
} else {
stop('Invalid format supplied, try "json" or "xml"')
}
resp
}
# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")
rev_history <- function(title, format = "json", n = 5){
base_url <- "https://en.wikipedia.org/w/api.php"
resp <- GET(base_url, query = list(action = "query", titles = title, prop = "revisions",
rvprop = "timestamp|user|comment|content", rvlimit = n,
format = format, rvdir="newer", rvstart = "2015-01-14T17:12:45Z", rvsection = 0))
resp }
had_json <- rev_history("Hadley Wickham")
saveRDS(had_json, file = "datasets/had_rev_json.rds")
rev_history <- function(title, format = "json", n = 5){
base_url <- "https://en.wikipedia.org/w/api.php"
resp <- GET(base_url, query = list(action = "query", titles = title, prop = "revisions",
rvprop = "timestamp|user|comment|content", rvlimit = n,
format = format, rvdir="newer", rvstart = "2015-01-14T17:12:45Z", rvsection = 0))
resp }
had_json <- rev_history("Hadley Wickham")
saveRDS(had_json, file = "had_rev_json.rds")
had_xml <- rev_history("Hadley Wickham", format = "xml", n = 5)
saveRDS(had_xml, file = "had_rev_xml.rds")
rev_history <- function(title, format = "json"){
if (title != "Hadley Wickham") {
stop('rev_history() only works for `title = "Hadley Wickham"`')
}
if (format == "json"){
resp <- readRDS("had_rev_json.rds")
} else if (format == "xml"){
resp <- readRDS("had_rev_xml.rds")
} else {
stop('Invalid format supplied, try "json" or "xml"')
}
resp
}
# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")
# Check http_type() of resp_json
http_type(resp_json)
# Examine returned text with content()
content(resp_json, as = "text")
# Parse response with content()
content(resp_json, as = "parsed")
# Parse returned text with fromJSON()
library(jsonlite)
fromJSON(content(resp_json, as = "text"))
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# install.packages("birdnik")
install.packages("rlist")
library("rlist")
packrat::snapshot()
# Load rlist
library(rlist)
# Examine output of this code
str(content(resp_json), max.level = 4)
# Store revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions
# Extract the user element
user_time <- list.select(revs, user, timestamp)
# Print user_time
user_time
# Stack to turn into a data frame
list.stack(user_time)
# Load rlist
library(rlist)
# Examine output of this code
str(content(resp_json), max.level = 4)
# Store revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions
# Extract the user element
user_time <- list.select(revs, user, timestamp)
# Print user_time
user_time
# Stack to turn into a data frame
list.stack(user_time)
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# install.packages("birdnik")
#install.packages("rlist")
install.packages("dplyr")
packrat::snapshot()
library("dplyr")
# Load dplyr
library(dplyr)
# Pull out revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions
# Extract user and timestamp
revs %>%
bind_rows %>%
select(user, timestamp)
revs %>%
bind_rows
revs
# Load dplyr
library(dplyr)
# Pull out revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions
# Extract user and timestamp
revs %>%
bind_rows %>%
select(user, timestamp)
# Load xml2
library(xml2)
# Get XML revision history
resp_xml <- rev_history("Hadley Wickham", format = "xml")
# Check response is XML
http_type(resp_xml)
# Examine returned text with content()
rev_text <- content(resp_xml, as = "text")
rev_text
# Turn rev_text into an XML document
rev_xml <- read_xml(rev_text)
# Examine the structure of rev_xml
xml_structure(rev_xml)
# Find all nodes using XPATH "/api/query/pages/page/revisions/rev"
xml_find_all(rev_xml, xpath ="/api/query/pages/page/revisions/rev")
# Find all rev nodes anywhere in document
rev_nodes <- xml_find_all(rev_xml, "//rev")
# Use xml_text() to get text from rev_nodes
xml_text(rev_nodes)
# All rev nodes
rev_nodes <- xml_find_all(rev_xml, "//rev")
# The first rev node
first_rev_node <- xml_find_first(rev_xml, "//rev")
# Find all attributes with xml_attrs()
xml_attrs(first_rev_node)
# Find user attribute with xml_attr()
xml_attr(first_rev_node, attr="user")
# Find user attribute for all rev nodes
xml_attr(rev_nodes, attr="user")
# Find anon attribute for all rev nodes
xml_attr(rev_nodes, attr = "anon")
get_revision_history <- function(article_title){
# Get raw revision response
rev_resp <- rev_history(article_title, format = "xml")
# Turn the content() of rev_resp into XML
rev_xml <- read_xml(content(rev_resp, "text"))
# Find revision nodes
rev_nodes <- xml_find_all(rev_xml, "//rev")
# Parse out usernames
user <- xml_attr(rev_nodes, attr = "user")
# Parse out timestamps
timestamp <- readr::parse_datetime(xml_attr(rev_nodes, "timestamp"))
# Parse out content
content <- xml_text(rev_nodes)
# Return data frame
data.frame(user = user,
timestamp = timestamp,
content = substr(content, 1, 40))
}
# Call function for "Hadley Wickham"
get_revision_history("Hadley Wickham")
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# install.packages("birdnik")
#install.packages("rlist")
# install.packages("dplyr")
install.packages("readr")
install.packages("readr")
library("readr")
remove.packages("readr", lib="")
