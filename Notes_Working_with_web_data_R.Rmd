---
title: "Working with Web Data in R"
author: "Alberto"
date: "27 de octubre de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# install.packages("birdnik")
#install.packages("rlist")
# install.packages("dplyr")
#install.packages("readr")

library("httr")
library("rvest")
library("jsonlite")
library("xml2")
library("birdnik")
library("rlist")
library("dplyr")
library("readr")
# download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_json.rds",  "had_rev_json.rds")
# download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_xml.rds", "had_rev_xml.rds")


```


# Course Description

Most of the useful data in the world, from economic data to news content to geographic information, lives somewhere on the internet - and this course will teach you how to access it. You'll explore how to work with APIs (computer-readable interfaces to websites), access data from Wikipedia and other sources, and build your own simple API client. For those occasions where APIs are not available, you'll find out how to use R to scrape information out of web pages. In the process you'll learn how to get data out of even the most stubborn website, and how to turn it into a format ready for further analysis. The packages you'll use and learn your way around are rvest, httr, xml2 and jsonlite, along with particular API client packages like WikipediR and pageviews.

## Downloading Files and Using API Clients

Sometimes getting data off the internet is very, very simple - it's stored in a format that R can handle and just lives on a server somewhere, or it's in a more complex format and perhaps part of an API but there's an R package designed to make using it a piece of cake. This chapter will explore how to download and read in static files, and how to use APIs when pre-existing clients are available.



### Downloading files and reading them into R

In this first exercise we're going to look at reading already-formatted datasets - CSV or TSV files, with which you'll no doubt be familiar! - into R from the internet. This is a lot easier than it might sound because R's file-reading functions accept not just file paths, but also URLs.

Instructions
The URLs to those files are in your R session as csv_url and tsv_url.

Read the CSV file stored at csv_url into R using read.csv(). Assign the result to csv_data.
Do the same for the TSV file stored at tsv_url using read.delim(). Assign the result to tsv_data.
Examine each object using head().

```{r}

# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"

# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)

# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)

# Examine the objects with head()
head(csv_data)
head(tsv_data)

```

## Saving raw files to disk

Sometimes just reading the file in from the web is enough, but often you'll want to store it locally so that you can refer back to it. This also lets you avoid having to spend the start of every analysis session twiddling your thumbs while particularly large files download.

Helpfully, R has download.file(), a function that lets you do just that: download a file to a location of your choice on your computer. It takes two arguments; url, indicating the URL to read from, and destfile, the destination to write the downloaded file to. In this case, we've pre-defined the URL - once again, it's csv_url.

Instructions
Download the file at csv_url with download.file(), naming the destination file "feed_data.csv".
Read "feed_data.csv" into R with read.csv().

```{r}
# Download the file with download.file()
# download.file(url = csv_url, destfile = "feed_data.csv")

# Read it in with read.csv()
csv_data <- read.csv("feed_data.csv")
```


### Saving formatted files to disk

Whether you're downloading the raw files with download.file() or using read.csv() and its sibling functions, at some point you're probably going to find the need to modify your input data, and then save the modified data to disk so you don't lose the changes.

You could use write.table(), but then you have to worry about accidentally writing out data in a format R can't read back in. An easy way to avoid this risk is to use saveRDS() and readRDS(), which save R objects in an R-specific file format, with the data structure intact. That means you can use it for any type of R object (even ones that don't turn into tables easily), and not worry you'll lose data reading it back in. saveRDS() takes two arguments, object, pointing to the R object to save and file pointing to where to save it to. readRDS() expects file, referring to the path to the RDS file to read in.

In this example we're going to modify the data you already read in, which is predefined as csv_data, and write the modified version out to a file before reading it in again.

Instructions
Modify csv_data to add the column square_weight, containing the square of the weight column.
Save it to disk as "modified_feed_data.RDS" with saveRDS().
Read it back in as modified_feed_data with readRDS().
Examine modified_feed_data.

```{r}
# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight * csv_data$weight

# Save it to disk with saveRDS()
saveRDS(csv_data, file = "modified_feed_data.RDS")

# Read it back in with readRDS()
modified_feed_data <- readRDS("modified_feed_data.RDS")

# Examine modified_feed_data
str(modified_feed_data)
```



### Using API clients

So we know that APIs are server components to make it easy for your code to interact with a service and get data from it. We also know that R features many "clients" - packages that wrap around connections to APIs so you don't have to worry about the details.

Let's look at a really simple API client - the pageviews package, which acts as a client to Wikipedia's API of pageview data. As with other R API clients, it's formatted as a package, and lives on CRAN - the central repository of R packages. The goal here is just to show how simple clients are to use: they look just like other R code, because they are just like other R code.

Instructions
Load the package pageviews.
Use the article_pageviews() function to get the pageviews for the article "Hadley Wickham".
Examine the resulting object.


```{r}
# Load pageviews
library("pageviews")

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)
```


Using access tokens
100xp
As we discussed in the last video, it's common for APIs to require access tokens - unique keys that verify you're authorised to use a service. They're usually pretty easy to use with an API client.

To show how they work, and how easy it can be, we're going to use the R client for the Wordnik dictionary and word use service - 'birdnik' - and an API token we prepared earlier. Birdnik is fairly simple (I wrote it!) and lets you get all sorts of interesting information about word usage in published works. For example, to get the frequency of the use of the word "chocolate", you would write:

word_frequency(api_key, "chocolate")
In this exercise we're going to look at the word "vector" (since it's a common word in R!) using a pre-existing API key (stored as api_key)

Instructions
Load the package birdnik.
Using the pre-existing API key and word_frequency(), get the frequency of the word "vector" in Wordnik's database. Assign the results to vector_frequency

```{r}

# Load birdnik
library("birdnik")

api_key <- "d8ed66f01da01b0c6a0070d7c1503801993a39c126fbc3382"


# Get the word frequency for "vector", using api_key to access it
vector_frequency <- word_frequency(api_key, "vector")

```



## Using httr to interact with APIs directly

If an API client doesn't exist, it's up to you to communicate directly with the API. But don't worry, the package `httr` makes this really straightforward. In this chapter you'll learn how to make web requests from R, how to examine the responses you get back and some best practices for doing this in a responsible way.


### GET requests in practice

To start with you're going to make a GET request. As discussed in the video, this is a request that asks the server to give you a particular piece of data or content (usually specified in the URL). These make up the majority of the requests you'll make in a data science context, since most of the time you'll be getting data from servers, not giving it to them.

To do this you'll use the httr package, written by Hadley Wickham (of course), which makes HTTP requests extremely easy. You're going to make a very simple GET request, and then inspect the output to see what it looks like.

Instructions
Load the httr package.
Use the GET() function to make a request to http://httpbin.org/get, saving the result to get_result.
Print get_result to inspect it.


```{r}
  # Load the httr package
library("httr")

# Make a GET request to http://httpbin.org/get
get_result <- GET("http://httpbin.org/get")

# Print it to inspect it
get_result
  
```


### POST requests in practice

Next we'll look at POST requests, also made through httr, with the function (you've guessed it) POST(). Rather than asking the server to give you something, as in GET requests, a POST request asks the server to accept something from you. They're commonly used for things like file upload, or authentication. As a result of their use for uploading things, POST() accepts not just a url but also a body argument containing whatever you want to give to the server.

You'll make a very simple POST request, just uploading a piece of text, and then inspect the output to see what it looks like.

Instructions
Load the httr package.
Make a POST request with the URL http://httpbin.org/post and the body "this is a test", saving the result to post_result.
Print post_result to inspect it.

```{r}
  # Load the httr package
library("httr")

# Make a POST request to http://httpbin.org/post with the body "this is a test"
post_result <- POST("http://httpbin.org/post", body = "this is a test")

# Print it to inspect it
post_result      

```

Nicely done. The output for POST requests looks pretty similar to that for GET requests, although (in this case) the body of your message is included - this is a test. Again, we'll dig into certain elements of the output in just a bit.


### Extracting the response

Making requests is all well and good, but it's also not why you're here. What we really want to do is get the data the server sent back, which can be done with httr's content() function. You pass it an object returned from a GET (or POST, or DELETE, or...) call, and it spits out whatever the server actually sent in an R-compatible structure.

We're going to demonstrate that now, using a slightly more complicated URL than before - in fact, using a URL from the Wikimedia pageviews system you dealt with through the pageviews package, which is stored as url. Without looking too much at the structure for the time being (we'll get to that later) this request asks for the number of pageviews to the English-language Wikipedia's "Hadley Wickham" article on 1 and 2 January 2017.

Instructions
httr is loaded in your workspace.

Make a GET request using the url object as the URL. Save the results as pageview_response.
Call content() on pageview_response to retrieve the data the server has sent back. Save the data as pageview_data.
Examine pageview_data with str()

```{r}

url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102"
# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)
```


### Handling http failures

As mentioned, HTTP calls can go wrong. Handling that can be done with httr's http_error() function, which identifies whether a server response contains an error.

If the response does contain an error, calling http_error() over the response will produce TRUE; otherwise, FALSE. You can use this for really fine-grained control over results. For example, you could check whether the request contained an error, and (if so) issue a warning and re-try the request.

For now we'll try something a bit simpler - issuing a warning that something went wrong if http_error() returns TRUE, and printing the content if it doesn't.

Instructions
Make a httr GET() request to the URL stored as fake_url, and store the result as request_result.
If http_error() returns TRUE, use warning() to issue the warning "The request failed".
If not, use content() (as demonstrated in previous exercises) to print the contents of the result.


```{r}
  
fake_url <- "http://google.com/fakepagethatdoesnotexist"

# Make the GET request
request_result <- GET(fake_url)

# Check request_result
if(http_error(request_result)){
	TRUE
	warning("The request failed")
} else {
	content(request_result)
}
```


Constructing queries (Part I)
100xp
As briefly discussed in the previous video, the actual API query (which tells the API what you want to do) tends to be in one of the two forms. The first is directory-based, where values are separated by / marks within the URL. The second is parameter-based, where all the values exist at the end of the URL and take the form of key=value.

Constructing directory-based URLs can be done via paste(), which takes an unlimited number of strings, along with a separator, as sep. So to construct http://swapi.co/api/vehicles/12 we'd call:

paste("http://swapi.co", "api", "vehicles", "12", sep = "/")
Let's do that now! We'll cover parameter-based URLs later. In the mean time we can play with SWAPI, mentioned above, which is an API chock full of star wars data. This time, rather than a vehicle, we'll look for a person.

Instructions
httr is loaded in your workspace.

Construct a directory-based API URL to http://swapi.co/api, looking for person 1 in people.
Assign the URL to directory_url.
Use GET to make an API call with directory_url.

```{r}
# Construct a directory-based API URL to `http://swapi.co/api`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", 1, sep = "/")

# Make a GET call with it
result <- GET(directory_url)
```

### Constructing queries (Part II)

As mentioned (albeit briefly) in the last exercise, there are also parameter based URLs, where all the query values exist at the end of the URL and take the form of key=value - they look something like http://fakeurl.com/foo.php?country=spain&food=goulash

Constructing parameter-based URLs can also be done with paste(), but the easiest way to do it is with GET() and POST() themselves, which accept a query argument consisting of a list of keys and values. So, to continue with the food-based examples, we could construct fakeurl.com/api.php?fruit=peaches&day=thursday with:

GET("fakeurl.com/api.php", query = list(fruit = "peaches", day = "thursday"))
In this exercise you'll construct a call to https://httpbin.org/get?nationality=americans&country=antigua.

Instructions
Start by contructing the query_params list, with a nationality parameter of "americans" and a country parameter of "antigua".
Construct a parameter-based call to https://httpbin.org/get, using GET() passing query_params to the query arugment.
Print the response parameter_response.

```{r}
  # Create list with nationality and country elements
query_params <- list(nationality = "americans", 
    country = "antigua")
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET("https://httpbin.org/get", query = query_params)

# Print parameter_response
parameter_response
```

### Using user agents

As discussed in the video, informative user-agents are a good way of being respectful of the developers running the API you're interacting with. They make it easy for them to contact you in the event something goes wrong. I always try to include:

My email address;
A URL for the project the code is a part of, if it's got a URL.
Building user agents is done by passing a call to user_agent() into the GET() or POST() request; something like:

GET("http://url.goes.here/", user_agent("somefakeemail@domain.com http://project.website"))
In the event you don't have a website, a short one-sentence description of what the project is about serves pretty well.

Instructions
Make a GET() request to url.
Include a user agent that has a fake email address "my@email.address" followed by the sentence "this is a test".
Assign the response to server_response.


```{r}
  # Do not change the url
url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100"

# Add the email address and the test sentence inside user_agent()
server_response <- GET(url, user_agent("my@email.address this is a test"))
```


### Rate-limiting

The next stage of respectful API usage is rate-limiting: making sure you only make a certain number of requests to the server in a given time period. What limit is expected will vary from server to server, but the implementation is always pretty much the same and involves a call to Sys.sleep(). This function takes one argument, a number, which represents the number of seconds to "sleep" (pause) the R session for. So if you call Sys.sleep(15), it'll pause for 15 seconds before allowing further code to run.

As you can imagine, this is really useful for rate-limiting. Only allowed 4 requests a minute? No problem! Just pause for 15 seconds between each request and you're guaranteed to never exceed it. Let's demonstrate now by putting together a little loop that sends multiple requests, on a 10-second time delay.

Instructions
Construct a vector of 2 URLs, http://fakeurl.com/api/1.0/ and http://fakeurl.com/api/2.0/.
Write a for-loop that sends a GET() request to each one.
Ensure that the for-loop uses Sys.sleep() to delay for 5 seconds between request.

```{r}
# Construct a vector of 2 URLs
urls <- c("http://fakeurl.com/api/1.0/","http://fakeurl.com/api/2.0/")

for(url in urls){
    # Send a GET request to url
    result <- GET(url)
    # Delay for 5 seconds between requests
    Sys.sleep(5)
}  
```

### Tying it all together

Using everything that you learned in the chapter, let's make a simple replica of one of the 'pageviews' functions - building queries, sending GET requests (with an appropriate user agent) and handling the output in a fault-tolerant way. To do this, you will use the function stop(), which takes a string as an argument, and stops the execution of the program.

Instructions
Using the example function structure:

Construct a URL pointing at
https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/**YOUR_ARTICLE_TITLE**/daily/2015100100/2015103100
dynamically inserting **YOUR_ARTICLE_TITLE**.

Make the request with a user agent containing a fake email address followed by a sample string: ("my@email.com this is a test").
Check the result for errors with http_error(), throwing an alert of "the request failed" with stop() if there was one.
Extract the body of the request with content() before returning it.

```{r}
get_pageviews <- function(article_title){
    
                  url <- paste0("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents", 
                  article_title, 
                  "daily/2015100100/2015103100", sep = "/") 
                      response <- GET(url, user_agent("my@email.com this is a test")) 
                        if(http_error(response)){ 
                          stop("the request failed") 
                        } else { 
                            result <- content(response) 
                            return(result) 
                        }
}

```


## Handling JSON and XML

Sometimes data is a TSV or nice plaintext output. Sometimes it's XML and/or JSON. This chapter walks you through what JSON and XML are, how to convert them into R-like objects, and how to extract data from them. You'll practice by examining the revision history for a Wikipedia article retrieved from the Wikipedia API using httr, xml2 and jsonlite.

### Parsing JSON

While JSON is a useful format for sharing data, your first step will often be to parse it into an R object, so you can manipulate it with R.

The content() function in httr retrieves the content from a request. It takes an as argument that specifies the type of output to return. You've already seen that as = "text" will return the content as a character string which is useful for checking the content is as you expect.

If you don't specify as, the default as = "parsed" is used. In this case the type of content() will be guessed based on the header and content() will choose an appropriate parsing function. For JSON this function is fromJSON() from the jsonlite package. If you know your response is JSON, you may want to use fromJSON() directly.

To practice, you'll retrieve some revision history from the Wikipedia API, check it is JSON, then parse it into a list two ways.

Instructions
Get the revision history for the Wikipedia article for "Hadley Wickham", by calling rev_history("Hadley Wickham") (a function we have written for you), store it in resp_json.
Check the http_type() of resp_json, to confirm the API returned a JSON object.
You can't always trust a header, so check the content looks like JSON by calling content() on resp_json with an additional argument, as.
Parse resp_json using content() by explicitly setting as = "parsed".
Parse the returned text (from step 3) with fromJSON()

```{r}

rev_history <- function(title, format = "json", n = 5){
  base_url <- "https://en.wikipedia.org/w/api.php"
    resp <- GET(base_url, query = list(action = "query", titles = title, prop = "revisions",
                                       rvprop = "timestamp|user|comment|content", rvlimit = n, 
                                       format = format, rvdir="newer", rvstart = "2015-01-14T17:12:45Z",
                                       rvsection = 0))
    resp }
    
    had_json <- rev_history("Hadley Wickham")
    saveRDS(had_json, file = "had_rev_json.rds")
    had_xml <- rev_history("Hadley Wickham", format = "xml", n = 5)
    saveRDS(had_xml, file = "had_rev_xml.rds")
    # Rewritten to not actually call API
    # download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_json.rds", "had_rev_json.rds")
    # download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_xml.rds", "had_rev_xml.rds")    
    # 
rev_history <- function(title, format = "json"){
  if (title != "Hadley Wickham") {
    stop('rev_history() only works for `title = "Hadley Wickham"`')
  }
  
  if (format == "json"){
    resp <- readRDS("had_rev_json.rds")
  } else if (format == "xml"){
    resp <- readRDS("had_rev_xml.rds")
  } else {
    stop('Invalid format supplied, try "json" or "xml"')
  }
  resp  
}


# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")

# Check http_type() of resp_json
http_type(resp_json)

# Examine returned text with content()
content(resp_json, as = "text")

# Parse response with content()
content(resp_json, as = "parsed")

# Parse returned text with fromJSON()
library(jsonlite)
fromJSON(content(resp_json, as = "text"))
```

### Manipulating parsed JSON

As you saw in the video, the output from parsing JSON is a list. One way to extract relevant data from that list is to use a package specifically designed for manipulating lists, rlist.

rlist provides two particularly useful functions for selecting and combining elements from a list: list.select() and list.stack(). list.select() extracts sub-elements by name from each element in a list. For example using the parsed movies data from the video (movies_list), we might ask for the title and year elements from each element:

list.select(movies_list, title, year)
The result is still a list, that is where list.stack() comes in. It will stack the elements of a list into a data frame.

list.stack(
    list.select(movies_list, title, year)
)
In this exercise you'll use these rlist functions to create a data frame with the user and timestamp for each revision.

Instructions
First, you'll need to figure out where the revisions are. Examine the output from the str() call. Can you see where the list of 5 revisions is?
Store the revisions in revs.
Use list.select() to pull out the user and timestamp elements from each revision, store in user_time.
Print user_time to verify it's a list with one element for each revision.
Use list.stack() to stack the lists into a data frame.

```{r}
# Load rlist
library(rlist)

# Examine output of this code
str(content(resp_json), max.level = 4)

# Store revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract the user element
user_time <- list.select(revs, user, timestamp)

# Print user_time
user_time

# Stack to turn into a data frame
list.stack(user_time)
```

Reformatting JSON

Of course you don't have to use rlist. You can achieve the same thing by using functions from base R or the tidyverse. In this exercise you'll repeat the task of extracting the username and timestamp using the dplyr package which is part of the tidyverse.

Conceptually, you'll take the list of revisions, stack them into a data frame, then pull out the relevant columns.

dplyr's bind_rows() function takes a list and turns it into a data frame. Then you can use select() to extract the relevant columns. And of course if we can make use of the %>% (pipe) operator to chain them all together.

Try it!

Instructions
Pipe the list of revisions into bind_rows().
Use select() to extract the user and timestamp columns.

```{r}
# Load dplyr
library(dplyr)

# Pull out revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract user and timestamp
revs %>%
  bind_rows %>%           
  select(user, timestamp)
```

### Examining XML documents

Just like JSON, you should first verify the response is indeed XML with http_type() and by examining the result of content(r, as = "text"). Then you can turn the response into an XML document object with read_xml().

One benefit of using the XML document object is the available functions that help you explore and manipulate the document. For example xml_structure() will print a representation of the XML document that emphasizes the hierarchical structure by displaying the elements without the data.

In this exercise you'll grab the same revision history you've been working with as XML, and take a look at it with xml_structure().

Instructions
Get the XML version of the revision history for the Wikipedia article for "Hadley Wickham", by calling rev_history("Hadley Wickham", format = "xml"), store it in resp_xml.
Check the response type of resp_xml to confirm the API returned an XML object.
You can't always trust a header, so check the content looks like XML by calling content() on resp_xml with as = "text", store in rev_text.
Turn rev_text into an XML object with read_xml() from the xml2 package, store as rev_xml.
Call xml_structure() on rev_xml to see the structure of the returned XML. Can you see where the revisions are?

```{r}
# Load xml2
library(xml2)

# Get XML revision history
resp_xml <- rev_history("Hadley Wickham", format = "xml")

# Check response is XML 
http_type(resp_xml)

# Examine returned text with content()
rev_text <- content(resp_xml, as = "text")
rev_text

# Turn rev_text into an XML document
rev_xml <- read_xml(rev_text)

# Examine the structure of rev_xml
xml_structure(rev_xml)
```

### Extracting XML data

XPATHs are designed to specifying nodes in an XML document. Remember /node_name specifies nodes at the current level that have the tag node_name, where as //node_name specifies nodes at any level below the current level that have the tag node_name.

xml2 provides the function xml_find_all() to extract nodes that match a given XPATH. For example, xml_find_all(rev_xml, "/api") will find all the nodes at the top level of the rev_xml document that have the tag api. Try running that in the console. You'll get a nodeset of one node because there is only one node that satisfies that XPATH.

The object returned from xml_find_all() is a nodeset (think of it like a list of nodes). To actually get data out of the nodes in the nodeset, you'll have to explicitly ask for it with xml_text() (or xml_double() or xml_integer()).

Use what you know about the location of the revisions data in the returned XML document extract just the content of the revision.

Instructions
Use xml_find_all() on rev_xml to find all the nodes that describe revisions by using the XPATH, "/api/query/pages/page/revisions/rev".
Use xml_find_all() on rev_xml to find all the nodes that are in a rev node anywhere in the document, store in rev_nodes.
Extract the contents from each node in rev_nodes, by passing rev_nodes to xml_text().

```{r}
# Find all nodes using XPATH "/api/query/pages/page/revisions/rev"
xml_find_all(rev_xml, xpath ="/api/query/pages/page/revisions/rev")

# Find all rev nodes anywhere in document
rev_nodes <- xml_find_all(rev_xml, "//rev")

# Use xml_text() to get text from rev_nodes
xml_text(rev_nodes)
```

### Extracting XML attributes

Not all the useful data will be in the content of a node, some might also be in the attributes of a node. To extract attributes from a nodeset, xml2 provides xml_attrs() and xml_attr().

xml_attrs() takes a nodeset and returns all of the attributes for every node in the nodeset. xml_attr() takes a nodeset and an additional argument attr to extract a single named argument from each node in the nodeset.

In this exercise you'll grab the user and anon attributes for each revision. You'll see xml_find_first() in the sample code. It works just like xml_find_all() but it only extracts the first node it finds.

Instructions
We've extracted rev_nodes and first_rev_node in the document for you to explore the difference between xml_attrs() and xml_attr().

Use xml_attrs() on first_rev_node to see all the attributes of the first revision node.
Use xml_attr() on first_rev_node along with an appropriate attr argument to extract the user attribute from the first revision node.
Now use xml_attr() again, but this time on rev_nodes to extract the user attribute from all revision nodes.
Use xml_attr() on rev_nodes to extract the anon attribute from all revision nodes.



```{r}
# All rev nodes
rev_nodes <- xml_find_all(rev_xml, "//rev")
rev_nodes
# The first rev node
first_rev_node <- xml_find_first(rev_xml, "//rev")
first_rev_node
# Find all attributes with xml_attrs()
xml_attrs(first_rev_node)

# Find user attribute with xml_attr()
xml_attr(first_rev_node, attr="user")

# Find user attribute for all rev nodes
xml_attr(rev_nodes, attr="user")

# Find anon attribute for all rev nodes
xml_attr(rev_nodes, attr = "anon")
```

Wrapup: returning nice API output

How might all this work together? A useful API function will retrieve results from an API and return them in a useful form. In Chapter 2, you finished up by writing a function that retrieves data from an API that relied on content() to convert it to a useful form. To write a more robust API function you shouldn't rely on content() but instead parse the data yourself.

To finish up this chapter you'll do exactly that: write get_revision_history() which retrieves the XML data for the revision history of page on Wikipedia, parses it, and returns it in a nice data frame.

So that you can focus on the parts of the function that parse the return object, you'll see your function calls rev_history() to get the response from the API. You can assume this function returns the raw response and follows the best practices you learnt in Chapter 2, like using a user agent, and checking the response status.

Instructions
Fill in the ___ to finish the function definition.

Use read_xml() to turn the content() of rev_resp as text into an XML object.
Use xml_find_all() to find all the rev nodes in the XML.
Parse out the "user" attribute from rev_nodes.
Parse out the content from rev_nodes using xml_text().
Finally, call get_revision_history() with article_title = "Hadley Wickham".

```{r}
get_revision_history <- function(article_title){
  # Get raw revision response
  rev_resp <- rev_history(article_title, format = "xml")
  
  # Turn the content() of rev_resp into XML
  rev_xml <- read_xml(content(rev_resp, "text"))
  
  # Find revision nodes
  rev_nodes <- xml_find_all(rev_xml, "//rev")

  # Parse out usernames
  user <- xml_attr(rev_nodes, attr = "user")
  
  # Parse out timestamps
  timestamp <- readr::parse_datetime(xml_attr(rev_nodes, "timestamp"))
  
  # Parse out content
  content <- xml_text(rev_nodes)
  
  # Return data frame 
  data.frame(user = user,
    timestamp = timestamp,
    content = substr(content, 1, 40))
}

# Call function for "Hadley Wickham"
get_revision_history("Hadley Wickham")
```


## Web scraping with XPATHs

Now that we've covered the low-hanging fruit ("it has an API, and a client", "it has an API") it's time to talk about what to do when a website doesn't have any access mechanisms at all - when you have to rely on web scraping. This chapter will introduce you to the rvest web-scraping package, and build on your previous knowledge of XML manipulation and XPATHs.

CSS Web Scraping and Final Case Study

CSS path-based web scraping is a far-more-pleasant alternative to using XPATHs. You'll start this chapter by learning about CSS, and how to leverage it for web scraping. Then, you'll work through a final case study that combines everything you've learnt so far to write a function that queries an API, parses the response and returns 
