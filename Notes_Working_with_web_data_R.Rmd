---
title: "Working with Web Data in R"
author: "Alberto"
date: "27 de octubre de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
# install.packages("httr")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("xml2")
# install.packages("birdnik")
#install.packages("rlist")
# install.packages("dplyr")
#install.packages("readr")

library("httr")
library("xml2")
library("rvest")
library("jsonlite")
library("birdnik")
library("rlist")
library("dplyr")
library("readr")
# download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_json.rds",  "had_rev_json.rds")
# download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_xml.rds", "had_rev_xml.rds")


```


# Course Description

Most of the useful data in the world, from economic data to news content to geographic information, lives somewhere on the internet - and this course will teach you how to access it. You'll explore how to work with APIs (computer-readable interfaces to websites), access data from Wikipedia and other sources, and build your own simple API client. For those occasions where APIs are not available, you'll find out how to use R to scrape information out of web pages. In the process you'll learn how to get data out of even the most stubborn website, and how to turn it into a format ready for further analysis. The packages you'll use and learn your way around are rvest, httr, xml2 and jsonlite, along with particular API client packages like WikipediR and pageviews.

## Downloading Files and Using API Clients

Sometimes getting data off the internet is very, very simple - it's stored in a format that R can handle and just lives on a server somewhere, or it's in a more complex format and perhaps part of an API but there's an R package designed to make using it a piece of cake. This chapter will explore how to download and read in static files, and how to use APIs when pre-existing clients are available.



### Downloading files and reading them into R

In this first exercise we're going to look at reading already-formatted datasets - CSV or TSV files, with which you'll no doubt be familiar! - into R from the internet. This is a lot easier than it might sound because R's file-reading functions accept not just file paths, but also URLs.

Instructions
The URLs to those files are in your R session as csv_url and tsv_url.

Read the CSV file stored at csv_url into R using read.csv(). Assign the result to csv_data.
Do the same for the TSV file stored at tsv_url using read.delim(). Assign the result to tsv_data.
Examine each object using head().

```{r}

# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"

# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)

# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)

# Examine the objects with head()
head(csv_data)
head(tsv_data)

```

## Saving raw files to disk

Sometimes just reading the file in from the web is enough, but often you'll want to store it locally so that you can refer back to it. This also lets you avoid having to spend the start of every analysis session twiddling your thumbs while particularly large files download.

Helpfully, R has download.file(), a function that lets you do just that: download a file to a location of your choice on your computer. It takes two arguments; url, indicating the URL to read from, and destfile, the destination to write the downloaded file to. In this case, we've pre-defined the URL - once again, it's csv_url.

Instructions
Download the file at csv_url with download.file(), naming the destination file "feed_data.csv".
Read "feed_data.csv" into R with read.csv().

```{r}
# Download the file with download.file()
# download.file(url = csv_url, destfile = "feed_data.csv")

# Read it in with read.csv()
csv_data <- read.csv("feed_data.csv")
```


### Saving formatted files to disk

Whether you're downloading the raw files with download.file() or using read.csv() and its sibling functions, at some point you're probably going to find the need to modify your input data, and then save the modified data to disk so you don't lose the changes.

You could use write.table(), but then you have to worry about accidentally writing out data in a format R can't read back in. An easy way to avoid this risk is to use saveRDS() and readRDS(), which save R objects in an R-specific file format, with the data structure intact. That means you can use it for any type of R object (even ones that don't turn into tables easily), and not worry you'll lose data reading it back in. saveRDS() takes two arguments, object, pointing to the R object to save and file pointing to where to save it to. readRDS() expects file, referring to the path to the RDS file to read in.

In this example we're going to modify the data you already read in, which is predefined as csv_data, and write the modified version out to a file before reading it in again.

Instructions
Modify csv_data to add the column square_weight, containing the square of the weight column.
Save it to disk as "modified_feed_data.RDS" with saveRDS().
Read it back in as modified_feed_data with readRDS().
Examine modified_feed_data.

```{r}
# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight * csv_data$weight

# Save it to disk with saveRDS()
saveRDS(csv_data, file = "modified_feed_data.RDS")

# Read it back in with readRDS()
modified_feed_data <- readRDS("modified_feed_data.RDS")

# Examine modified_feed_data
str(modified_feed_data)
```



### Using API clients

So we know that APIs are server components to make it easy for your code to interact with a service and get data from it. We also know that R features many "clients" - packages that wrap around connections to APIs so you don't have to worry about the details.

Let's look at a really simple API client - the pageviews package, which acts as a client to Wikipedia's API of pageview data. As with other R API clients, it's formatted as a package, and lives on CRAN - the central repository of R packages. The goal here is just to show how simple clients are to use: they look just like other R code, because they are just like other R code.

Instructions
Load the package pageviews.
Use the article_pageviews() function to get the pageviews for the article "Hadley Wickham".
Examine the resulting object.


```{r}
# Load pageviews
library("pageviews")

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)
```


Using access tokens
100xp
As we discussed in the last video, it's common for APIs to require access tokens - unique keys that verify you're authorised to use a service. They're usually pretty easy to use with an API client.

To show how they work, and how easy it can be, we're going to use the R client for the Wordnik dictionary and word use service - 'birdnik' - and an API token we prepared earlier. Birdnik is fairly simple (I wrote it!) and lets you get all sorts of interesting information about word usage in published works. For example, to get the frequency of the use of the word "chocolate", you would write:

word_frequency(api_key, "chocolate")
In this exercise we're going to look at the word "vector" (since it's a common word in R!) using a pre-existing API key (stored as api_key)

Instructions
Load the package birdnik.
Using the pre-existing API key and word_frequency(), get the frequency of the word "vector" in Wordnik's database. Assign the results to vector_frequency

```{r}

# Load birdnik
library("birdnik")

api_key <- "d8ed66f01da01b0c6a0070d7c1503801993a39c126fbc3382"


# Get the word frequency for "vector", using api_key to access it
vector_frequency <- word_frequency(api_key, "vector")

```



## Using httr to interact with APIs directly

If an API client doesn't exist, it's up to you to communicate directly with the API. But don't worry, the package `httr` makes this really straightforward. In this chapter you'll learn how to make web requests from R, how to examine the responses you get back and some best practices for doing this in a responsible way.


### GET requests in practice

To start with you're going to make a GET request. As discussed in the video, this is a request that asks the server to give you a particular piece of data or content (usually specified in the URL). These make up the majority of the requests you'll make in a data science context, since most of the time you'll be getting data from servers, not giving it to them.

To do this you'll use the httr package, written by Hadley Wickham (of course), which makes HTTP requests extremely easy. You're going to make a very simple GET request, and then inspect the output to see what it looks like.

Instructions
Load the httr package.
Use the GET() function to make a request to http://httpbin.org/get, saving the result to get_result.
Print get_result to inspect it.


```{r}
  # Load the httr package
library("httr")

# Make a GET request to http://httpbin.org/get
get_result <- GET("http://httpbin.org/get")

# Print it to inspect it
get_result
  
```


### POST requests in practice

Next we'll look at POST requests, also made through httr, with the function (you've guessed it) POST(). Rather than asking the server to give you something, as in GET requests, a POST request asks the server to accept something from you. They're commonly used for things like file upload, or authentication. As a result of their use for uploading things, POST() accepts not just a url but also a body argument containing whatever you want to give to the server.

You'll make a very simple POST request, just uploading a piece of text, and then inspect the output to see what it looks like.

Instructions
Load the httr package.
Make a POST request with the URL http://httpbin.org/post and the body "this is a test", saving the result to post_result.
Print post_result to inspect it.

```{r}
  # Load the httr package
library("httr")

# Make a POST request to http://httpbin.org/post with the body "this is a test"
post_result <- POST("http://httpbin.org/post", body = "this is a test")

# Print it to inspect it
post_result      

```

Nicely done. The output for POST requests looks pretty similar to that for GET requests, although (in this case) the body of your message is included - this is a test. Again, we'll dig into certain elements of the output in just a bit.


### Extracting the response

Making requests is all well and good, but it's also not why you're here. What we really want to do is get the data the server sent back, which can be done with httr's content() function. You pass it an object returned from a GET (or POST, or DELETE, or...) call, and it spits out whatever the server actually sent in an R-compatible structure.

We're going to demonstrate that now, using a slightly more complicated URL than before - in fact, using a URL from the Wikimedia pageviews system you dealt with through the pageviews package, which is stored as url. Without looking too much at the structure for the time being (we'll get to that later) this request asks for the number of pageviews to the English-language Wikipedia's "Hadley Wickham" article on 1 and 2 January 2017.

Instructions
httr is loaded in your workspace.

Make a GET request using the url object as the URL. Save the results as pageview_response.
Call content() on pageview_response to retrieve the data the server has sent back. Save the data as pageview_data.
Examine pageview_data with str()

```{r}

url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102"
# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)
```


### Handling http failures

As mentioned, HTTP calls can go wrong. Handling that can be done with httr's http_error() function, which identifies whether a server response contains an error.

If the response does contain an error, calling http_error() over the response will produce TRUE; otherwise, FALSE. You can use this for really fine-grained control over results. For example, you could check whether the request contained an error, and (if so) issue a warning and re-try the request.

For now we'll try something a bit simpler - issuing a warning that something went wrong if http_error() returns TRUE, and printing the content if it doesn't.

Instructions
Make a httr GET() request to the URL stored as fake_url, and store the result as request_result.
If http_error() returns TRUE, use warning() to issue the warning "The request failed".
If not, use content() (as demonstrated in previous exercises) to print the contents of the result.


```{r}
  
fake_url <- "http://google.com/fakepagethatdoesnotexist"

# Make the GET request
request_result <- GET(fake_url)

# Check request_result
if(http_error(request_result)){
	TRUE
	warning("The request failed")
} else {
	content(request_result)
}
```


Constructing queries (Part I)
100xp
As briefly discussed in the previous video, the actual API query (which tells the API what you want to do) tends to be in one of the two forms. The first is directory-based, where values are separated by / marks within the URL. The second is parameter-based, where all the values exist at the end of the URL and take the form of key=value.

Constructing directory-based URLs can be done via paste(), which takes an unlimited number of strings, along with a separator, as sep. So to construct http://swapi.co/api/vehicles/12 we'd call:

paste("http://swapi.co", "api", "vehicles", "12", sep = "/")
Let's do that now! We'll cover parameter-based URLs later. In the mean time we can play with SWAPI, mentioned above, which is an API chock full of star wars data. This time, rather than a vehicle, we'll look for a person.

Instructions
httr is loaded in your workspace.

Construct a directory-based API URL to http://swapi.co/api, looking for person 1 in people.
Assign the URL to directory_url.
Use GET to make an API call with directory_url.

```{r}
# Construct a directory-based API URL to `http://swapi.co/api`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", 1, sep = "/")

# Make a GET call with it
result <- GET(directory_url)
```

### Constructing queries (Part II)

As mentioned (albeit briefly) in the last exercise, there are also parameter based URLs, where all the query values exist at the end of the URL and take the form of key=value - they look something like http://fakeurl.com/foo.php?country=spain&food=goulash

Constructing parameter-based URLs can also be done with paste(), but the easiest way to do it is with GET() and POST() themselves, which accept a query argument consisting of a list of keys and values. So, to continue with the food-based examples, we could construct fakeurl.com/api.php?fruit=peaches&day=thursday with:

GET("fakeurl.com/api.php", query = list(fruit = "peaches", day = "thursday"))
In this exercise you'll construct a call to https://httpbin.org/get?nationality=americans&country=antigua.

Instructions
Start by contructing the query_params list, with a nationality parameter of "americans" and a country parameter of "antigua".
Construct a parameter-based call to https://httpbin.org/get, using GET() passing query_params to the query arugment.
Print the response parameter_response.

```{r}
  # Create list with nationality and country elements
query_params <- list(nationality = "americans", 
    country = "antigua")
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET("https://httpbin.org/get", query = query_params)

# Print parameter_response
parameter_response
```

### Using user agents

As discussed in the video, informative user-agents are a good way of being respectful of the developers running the API you're interacting with. They make it easy for them to contact you in the event something goes wrong. I always try to include:

My email address;
A URL for the project the code is a part of, if it's got a URL.
Building user agents is done by passing a call to user_agent() into the GET() or POST() request; something like:

GET("http://url.goes.here/", user_agent("somefakeemail@domain.com http://project.website"))
In the event you don't have a website, a short one-sentence description of what the project is about serves pretty well.

Instructions
Make a GET() request to url.
Include a user agent that has a fake email address "my@email.address" followed by the sentence "this is a test".
Assign the response to server_response.


```{r}
  # Do not change the url
url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100"

# Add the email address and the test sentence inside user_agent()
server_response <- GET(url, user_agent("my@email.address this is a test"))
```


### Rate-limiting

The next stage of respectful API usage is rate-limiting: making sure you only make a certain number of requests to the server in a given time period. What limit is expected will vary from server to server, but the implementation is always pretty much the same and involves a call to Sys.sleep(). This function takes one argument, a number, which represents the number of seconds to "sleep" (pause) the R session for. So if you call Sys.sleep(15), it'll pause for 15 seconds before allowing further code to run.

As you can imagine, this is really useful for rate-limiting. Only allowed 4 requests a minute? No problem! Just pause for 15 seconds between each request and you're guaranteed to never exceed it. Let's demonstrate now by putting together a little loop that sends multiple requests, on a 10-second time delay.

Instructions
Construct a vector of 2 URLs, http://fakeurl.com/api/1.0/ and http://fakeurl.com/api/2.0/.
Write a for-loop that sends a GET() request to each one.
Ensure that the for-loop uses Sys.sleep() to delay for 5 seconds between request.

```{r}
# Construct a vector of 2 URLs
urls <- c("http://fakeurl.com/api/1.0/","http://fakeurl.com/api/2.0/")

for(url in urls){
    # Send a GET request to url
    result <- GET(url)
    # Delay for 5 seconds between requests
    Sys.sleep(5)
}  
```

### Tying it all together

Using everything that you learned in the chapter, let's make a simple replica of one of the 'pageviews' functions - building queries, sending GET requests (with an appropriate user agent) and handling the output in a fault-tolerant way. To do this, you will use the function stop(), which takes a string as an argument, and stops the execution of the program.

Instructions
Using the example function structure:

Construct a URL pointing at
https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/**YOUR_ARTICLE_TITLE**/daily/2015100100/2015103100
dynamically inserting **YOUR_ARTICLE_TITLE**.

Make the request with a user agent containing a fake email address followed by a sample string: ("my@email.com this is a test").
Check the result for errors with http_error(), throwing an alert of "the request failed" with stop() if there was one.
Extract the body of the request with content() before returning it.

```{r}
get_pageviews <- function(article_title){
    
                  url <- paste0("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents", 
                  article_title, 
                  "daily/2015100100/2015103100", sep = "/") 
                      response <- GET(url, user_agent("my@email.com this is a test")) 
                        if(http_error(response)){ 
                          stop("the request failed") 
                        } else { 
                            result <- content(response) 
                            return(result) 
                        }
}

```


## Handling JSON and XML

Sometimes data is a TSV or nice plaintext output. Sometimes it's XML and/or JSON. This chapter walks you through what JSON and XML are, how to convert them into R-like objects, and how to extract data from them. You'll practice by examining the revision history for a Wikipedia article retrieved from the Wikipedia API using httr, xml2 and jsonlite.

### Parsing JSON

While JSON is a useful format for sharing data, your first step will often be to parse it into an R object, so you can manipulate it with R.

The content() function in httr retrieves the content from a request. It takes an as argument that specifies the type of output to return. You've already seen that as = "text" will return the content as a character string which is useful for checking the content is as you expect.

If you don't specify as, the default as = "parsed" is used. In this case the type of content() will be guessed based on the header and content() will choose an appropriate parsing function. For JSON this function is fromJSON() from the jsonlite package. If you know your response is JSON, you may want to use fromJSON() directly.

To practice, you'll retrieve some revision history from the Wikipedia API, check it is JSON, then parse it into a list two ways.

Instructions
Get the revision history for the Wikipedia article for "Hadley Wickham", by calling rev_history("Hadley Wickham") (a function we have written for you), store it in resp_json.
Check the http_type() of resp_json, to confirm the API returned a JSON object.
You can't always trust a header, so check the content looks like JSON by calling content() on resp_json with an additional argument, as.
Parse resp_json using content() by explicitly setting as = "parsed".
Parse the returned text (from step 3) with fromJSON()

```{r}

rev_history <- function(title, format = "json", n = 5){
  base_url <- "https://en.wikipedia.org/w/api.php"
    resp <- GET(base_url, query = list(action = "query", titles = title, prop = "revisions",
                                       rvprop = "timestamp|user|comment|content", rvlimit = n, 
                                       format = format, rvdir="newer", rvstart = "2015-01-14T17:12:45Z",
                                       rvsection = 0))
    resp }
    
    had_json <- rev_history("Hadley Wickham")
    saveRDS(had_json, file = "had_rev_json.rds")
    had_xml <- rev_history("Hadley Wickham", format = "xml", n = 5)
    saveRDS(had_xml, file = "had_rev_xml.rds")
    # Rewritten to not actually call API
    # download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_json.rds", "had_rev_json.rds")
    # download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/had_rev_xml.rds", "had_rev_xml.rds")    
    # 
rev_history <- function(title, format = "json"){
  if (title != "Hadley Wickham") {
    stop('rev_history() only works for `title = "Hadley Wickham"`')
  }
  
  if (format == "json"){
    resp <- readRDS("had_rev_json.rds")
  } else if (format == "xml"){
    resp <- readRDS("had_rev_xml.rds")
  } else {
    stop('Invalid format supplied, try "json" or "xml"')
  }
  resp  
}


# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")

# Check http_type() of resp_json
http_type(resp_json)

# Examine returned text with content()
content(resp_json, as = "text")

# Parse response with content()
content(resp_json, as = "parsed")

# Parse returned text with fromJSON()
library(jsonlite)
fromJSON(content(resp_json, as = "text"))
```

### Manipulating parsed JSON

As you saw in the video, the output from parsing JSON is a list. One way to extract relevant data from that list is to use a package specifically designed for manipulating lists, rlist.

rlist provides two particularly useful functions for selecting and combining elements from a list: list.select() and list.stack(). list.select() extracts sub-elements by name from each element in a list. For example using the parsed movies data from the video (movies_list), we might ask for the title and year elements from each element:

list.select(movies_list, title, year)
The result is still a list, that is where list.stack() comes in. It will stack the elements of a list into a data frame.

list.stack(
    list.select(movies_list, title, year)
)
In this exercise you'll use these rlist functions to create a data frame with the user and timestamp for each revision.

Instructions
First, you'll need to figure out where the revisions are. Examine the output from the str() call. Can you see where the list of 5 revisions is?
Store the revisions in revs.
Use list.select() to pull out the user and timestamp elements from each revision, store in user_time.
Print user_time to verify it's a list with one element for each revision.
Use list.stack() to stack the lists into a data frame.

```{r}
# Load rlist
library(rlist)

# Examine output of this code
str(content(resp_json), max.level = 4)

# Store revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract the user element
user_time <- list.select(revs, user, timestamp)

# Print user_time
user_time

# Stack to turn into a data frame
list.stack(user_time)
```

Reformatting JSON

Of course you don't have to use rlist. You can achieve the same thing by using functions from base R or the tidyverse. In this exercise you'll repeat the task of extracting the username and timestamp using the dplyr package which is part of the tidyverse.

Conceptually, you'll take the list of revisions, stack them into a data frame, then pull out the relevant columns.

dplyr's bind_rows() function takes a list and turns it into a data frame. Then you can use select() to extract the relevant columns. And of course if we can make use of the %>% (pipe) operator to chain them all together.

Try it!

Instructions
Pipe the list of revisions into bind_rows().
Use select() to extract the user and timestamp columns.

```{r}
# Load dplyr
library(dplyr)

# Pull out revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract user and timestamp
revs %>%
  bind_rows %>%           
  select(user, timestamp)
```

### Examining XML documents

Just like JSON, you should first verify the response is indeed XML with http_type() and by examining the result of content(r, as = "text"). Then you can turn the response into an XML document object with read_xml().

One benefit of using the XML document object is the available functions that help you explore and manipulate the document. For example xml_structure() will print a representation of the XML document that emphasizes the hierarchical structure by displaying the elements without the data.

In this exercise you'll grab the same revision history you've been working with as XML, and take a look at it with xml_structure().

Instructions
Get the XML version of the revision history for the Wikipedia article for "Hadley Wickham", by calling rev_history("Hadley Wickham", format = "xml"), store it in resp_xml.
Check the response type of resp_xml to confirm the API returned an XML object.
You can't always trust a header, so check the content looks like XML by calling content() on resp_xml with as = "text", store in rev_text.
Turn rev_text into an XML object with read_xml() from the xml2 package, store as rev_xml.
Call xml_structure() on rev_xml to see the structure of the returned XML. Can you see where the revisions are?

```{r}
# Load xml2
library(xml2)

# Get XML revision history
resp_xml <- rev_history("Hadley Wickham", format = "xml")

# Check response is XML 
http_type(resp_xml)

# Examine returned text with content()
rev_text <- content(resp_xml, as = "text")
rev_text

# Turn rev_text into an XML document
rev_xml <- read_xml(rev_text)

# Examine the structure of rev_xml
xml_structure(rev_xml)
```

### Extracting XML data

XPATHs are designed to specifying nodes in an XML document. Remember /node_name specifies nodes at the current level that have the tag node_name, where as //node_name specifies nodes at any level below the current level that have the tag node_name.

xml2 provides the function xml_find_all() to extract nodes that match a given XPATH. For example, xml_find_all(rev_xml, "/api") will find all the nodes at the top level of the rev_xml document that have the tag api. Try running that in the console. You'll get a nodeset of one node because there is only one node that satisfies that XPATH.

The object returned from xml_find_all() is a nodeset (think of it like a list of nodes). To actually get data out of the nodes in the nodeset, you'll have to explicitly ask for it with xml_text() (or xml_double() or xml_integer()).

Use what you know about the location of the revisions data in the returned XML document extract just the content of the revision.

Instructions
Use xml_find_all() on rev_xml to find all the nodes that describe revisions by using the XPATH, "/api/query/pages/page/revisions/rev".
Use xml_find_all() on rev_xml to find all the nodes that are in a rev node anywhere in the document, store in rev_nodes.
Extract the contents from each node in rev_nodes, by passing rev_nodes to xml_text().

```{r}
# Find all nodes using XPATH "/api/query/pages/page/revisions/rev"
xml_find_all(rev_xml, xpath ="/api/query/pages/page/revisions/rev")

# Find all rev nodes anywhere in document
rev_nodes <- xml_find_all(rev_xml, "//rev")

# Use xml_text() to get text from rev_nodes
xml_text(rev_nodes)
```

### Extracting XML attributes

Not all the useful data will be in the content of a node, some might also be in the attributes of a node. To extract attributes from a nodeset, xml2 provides xml_attrs() and xml_attr().

xml_attrs() takes a nodeset and returns all of the attributes for every node in the nodeset. xml_attr() takes a nodeset and an additional argument attr to extract a single named argument from each node in the nodeset.

In this exercise you'll grab the user and anon attributes for each revision. You'll see xml_find_first() in the sample code. It works just like xml_find_all() but it only extracts the first node it finds.

Instructions
We've extracted rev_nodes and first_rev_node in the document for you to explore the difference between xml_attrs() and xml_attr().

Use xml_attrs() on first_rev_node to see all the attributes of the first revision node.
Use xml_attr() on first_rev_node along with an appropriate attr argument to extract the user attribute from the first revision node.
Now use xml_attr() again, but this time on rev_nodes to extract the user attribute from all revision nodes.
Use xml_attr() on rev_nodes to extract the anon attribute from all revision nodes.



```{r}
# All rev nodes
rev_nodes <- xml_find_all(rev_xml, "//rev")
rev_nodes
# The first rev node
first_rev_node <- xml_find_first(rev_xml, "//rev")
first_rev_node
# Find all attributes with xml_attrs()
xml_attrs(first_rev_node)

# Find user attribute with xml_attr()
xml_attr(first_rev_node, attr="user")

# Find user attribute for all rev nodes
xml_attr(rev_nodes, attr="user")

# Find anon attribute for all rev nodes
xml_attr(rev_nodes, attr = "anon")
```

Wrapup: returning nice API output

How might all this work together? A useful API function will retrieve results from an API and return them in a useful form. In Chapter 2, you finished up by writing a function that retrieves data from an API that relied on content() to convert it to a useful form. To write a more robust API function you shouldn't rely on content() but instead parse the data yourself.

To finish up this chapter you'll do exactly that: write get_revision_history() which retrieves the XML data for the revision history of page on Wikipedia, parses it, and returns it in a nice data frame.

So that you can focus on the parts of the function that parse the return object, you'll see your function calls rev_history() to get the response from the API. You can assume this function returns the raw response and follows the best practices you learnt in Chapter 2, like using a user agent, and checking the response status.

Instructions
Fill in the ___ to finish the function definition.

Use read_xml() to turn the content() of rev_resp as text into an XML object.
Use xml_find_all() to find all the rev nodes in the XML.
Parse out the "user" attribute from rev_nodes.
Parse out the content from rev_nodes using xml_text().
Finally, call get_revision_history() with article_title = "Hadley Wickham".

```{r}
get_revision_history <- function(article_title){
  # Get raw revision response
  rev_resp <- rev_history(article_title, format = "xml")
  
  # Turn the content() of rev_resp into XML
  rev_xml <- read_xml(content(rev_resp, "text"))
  
  # Find revision nodes
  rev_nodes <- xml_find_all(rev_xml, "//rev")

  # Parse out usernames
  user <- xml_attr(rev_nodes, attr = "user")
  
  # Parse out timestamps
  timestamp <- readr::parse_datetime(xml_attr(rev_nodes, "timestamp"))
  
  # Parse out content
  content <- xml_text(rev_nodes)
  
  # Return data frame 
  data.frame(user = user,
    timestamp = timestamp,
    content = substr(content, 1, 40))
}

# Call function for "Hadley Wickham"
get_revision_history("Hadley Wickham")
```


## Web scraping with XPATHs

Now that we've covered the low-hanging fruit ("it has an API, and a client", "it has an API") it's time to talk about what to do when a website doesn't have any access mechanisms at all - when you have to rely on web scraping. This chapter will introduce you to the rvest web-scraping package, and build on your previous knowledge of XML manipulation and XPATHs.

### Reading HTML

The first step with web scraping is actually reading the HTML in. This can be done with a function from xml2, which is imported by rvest - read_html(). This accepts a single URL, and returns a big blob of XML that we can use further on.

We're going to experiment with that by grabbing Hadley Wickham's wikipedia page, with rvest, and then printing it just to see what the structure looks like.

Instructions
Load the rvest package.
Use read_html() to read the URL stored at test_url. Store the results as test_xml.
Print test_xml.

```{r}
# Load rvest
library("rvest")

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

# Print test_xml
test_xml
```

### Extracting nodes by XPATH

Now you've got a HTML page read into R. Great! But how do you get individual, identifiable pieces of it?

The answer is to use html_node(), which extracts individual chunks of HTML from a HTML document. There are a couple of ways of identifying and filtering nodes, and for now we're going to use XPATHs: unique identifiers for individual pieces of a HTML document.

These can be retrieved using a browser gadget we'll talk about later - in the meanwhile the XPATH for the information box in the page you just downloaded is stored as test_node_xpath. We're going to retrieve the box from the HTML doc with html_node(), using test_node_xpath as the xpath argument.

Instructions
Use html_node() to retrieve the node with the XPATH stored at test_node_xpath from test_xml document you grabbed in the last exercise.
Print the first element of the results.

```{r}

test_node_xpath <- "//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"vcard\", \" \" ))]"
test_node_xpath

  # Use html_node() to grab the node with the XPATH stored as `test_node_xpath`
node <- html_node(x = test_xml, xpath = test_node_xpath)

# Print the first element of the result
node[1]
```

### Extracting names

The first thing we'll grab is a name, from the first element of the previously extracted table (now stored as table_element). We can do this with html_name(). As you may recall from when you printed it, the element has the tag <table>...</table>, so we'd expect the name to be, well, table.

Instructions
Extract the name of table_element using the function html_name(). Save it as element_name.
Print element_name.

```{r}
table_element <- html_node(x = test_xml, xpath = test_node_xpath)

# Extract the name of table_element
element_name <- html_name(table_element)

# Print the name
element_name
```

### Extracting values

Just knowing the type of HTML object a node is isn't much use, though (although it can be very helpful). What we really want is to extract the actual text stored within the value.

We can do that with (shocker) html_text(), another convenient rvest function that accepts a node and passes back the text inside it. For this we'll want a node within the extracted element - specifically, the one containing the page title. The xpath value for that node is stored as second_xpath_val.

Using this xpath value, extract the node within table_element that we want, and then use html_text to extract the text, before printing it.

Instructions
Extract the element of table_element referred to by second_xpath_val and store it as page_name.
Extract the text from page_name using html_text(), saving it as page_title.
Print page_title.

```{r}
second_xpath_val <- "//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"fn\", \" \" ))]"
second_xpath_val

# Extract the element of table_element referred to by second_xpath_val and store it as page_name
page_name <- html_node(x = table_element, xpath = second_xpath_val)

# Extract the text from page_name
page_title <- html_text(page_name)

# Print page_title
page_title
```

### Extracting tables

The data from Wikipedia that we've been playing around with can be extracted bit by bit and cleaned up manually, but since it's a table, we have an easier way of turning it into an R object. rvest contains the function html_table() which, as the name suggests, extracts tables. It accepts a node containing a table object, and outputs a data frame.

Let's use it now: take the table we've extracted, and turn it into a data frame.

Instructions
Turn table_element into a data frame and assign it to wiki_table.
Print the resulting object.

```{r}
# Turn table_element into a data frame and assign it to wiki_table
wiki_table <- html_table(table_element)

# Print wiki_table
wiki_table
```

### Cleaning a data frame

In the last exercise, we looked at extracting tables with html_table(). The resulting data frame was pretty clean, but had two problems - first, the column names weren't descriptive, and second, there was an empty row.

In this exercise we're going to look at fixing both of those problems. First, column names. Column names can be cleaned up with the colnames() function. You call it on the object you want to rename, and then assign to that call a vector of new names.

The missing row, meanwhile, can be removed with the subset() function. subset takes an object, and a condition. For example, if you have a data frame df containing a column x, you could run

subset(df, !x == "")
to remove all rows from df consisting of empty strings ("") in the column x.

Instructions
Rename the columns of wiki_table to "key" and "value" using colnames().
Remove the empty row from wiki_table using subset(), and assign the result to cleaned_table.
Print cleaned_table.

```{r}
# Rename the columns of wiki_table
colnames(wiki_table) <- c("key", "value")

# Remove the empty row from wiki_table
cleaned_table <- subset(wiki_table, !key == "")

# Print cleaned_table
cleaned_table
```

## CSS Web Scraping and Final Case Study

CSS path-based web scraping is a far-more-pleasant alternative to using XPATHs. You'll start this chapter by learning about CSS, and how to leverage it for web scraping. Then, you'll work through a final case study that combines everything you've learnt so far to write a function that queries an API, parses the response and returns 

### Using CSS to scrape nodes

As mentioned in the video, CSS is a way to add design information to HTML, that instructs the browser on how to display the content. You can leverage these design instructions to identify content on the page.

You've already used html_node(), but it's more common with CSS selectors to use html_nodes() since you'll often want more than one node returned. Both functions allow you to specify a css argument to use a CSS selector, instead of specifying the xpath argument.

What do CSS selectors look like? Try these examples to see a few possibilities.

Instructions
We've read in the same HTML page from Chapter 4, the Wikipedia page for Hadley Wickham, into test_xml.

Use the CSS selector "table" to select all elements that are a table tag.
Use the CSS selector ".infobox" to select all elements that have the attribute class = "infobox".
Use the CSS selector "#firstHeading" to select all elements that have the attribute id = "firstHeading".

```{r}
# Select the table elements
html_nodes(test_xml, css = "table")

# Select elements with class = "infobox"
html_nodes(test_xml, css = ".infobox")

# Select elements with id = "firstHeading"
html_nodes(test_xml, css = "#firstHeading")
```

### Scraping names

You might have noticed in the previous exercise, to select elements with a certain class, you add a . in front of the class name. If you need to select an element based on its id, you add a # in front of the id name.

For example if this element was inside your HTML document:

<h1 class = "heading" id = "intro">
  Introduction
</h1>
You could select it by its class using the CSS selector ".heading", or by its id using the CSS selector "#intro".

Once you've selected an element with a CSS selector, you can get the element tag name just like you did with XPATH selectors, with html_name(). Try it!

Instructions
The infobox you extracted in Chapter 4 has the class infobox. Use html_nodes() and the appropriate CSS selector to extract the infobox element to infobox_element.
Use html_name() to extract the tag name of infobox_element and store it in element_name.
Print element_name.

```{r}
# Extract element with class infobox
infobox_element <- html_nodes(test_xml, css = ".infobox")

# Get tag name of infobox_element
element_name <- html_name(infobox_element)

# Print element_name
element_name
```

### Scraping text

Of course you can get the contents of a node extracted using a CSS selector too, with html_text().

Can you put the pieces together to get the page title like you did in Chapter 4?

Instructions
The infobox HTML element is stored in infobox_element in your workspace.

Use html_node() to extract the element from infobox_element with the CSS class fn.
Use html_text() to extract the contents of page_name.
Print page_title.

```{r}
# Extract element with class fn
page_name <- html_node(x = infobox_element, css=".fn")

# Get contents of page_name
page_title <-html_text(page_name)

# Print page_title
page_title
```

### Test: CSS web scraping

Take a look at the chunk of HTML being read into test:

test <- read_html('
   <h1 class = "main">Hello world!</h1>
   ')
How would you extract the text Hello world! using rvest and CSS selectors?

Possible Answers
html_text(html_node(test, css = ".main"))

API calls

Your first step is to use the Wikipedia API to get the page contents for a specific page. We'll continue to work with the Hadley Wickham page, but as your last exercise, you'll make it more general.

To get the content of a page from the Wikipedia API you need to use a parameter based URL. The URL you want is

https://en.wikipedia.org/w/api.php?action=parse&page=Hadley%20Wickham&format=xml
which specifies that you want the parsed content (i.e the HTML) for the "Hadley Wickham" page, and the API response should be XML.

In this exercise you'll make the request with GET() and parse the XML response with content().

Instructions
We've already defined base_url for you.

Create a list for the query parameters, setting action = "parse", page = "Hadley Wickham" and format = "xml".
Use GET() to call the API by specifying url and query.
Parse the response using content().

```{r}
# Load httr
library(httr)

# The API url
base_url <- "https://en.wikipedia.org/w/api.php"

# Set query parameters
query_params <- list(action="parse", 
  page="Hadley Wickham", 
  format="xml")

# Get data from API
resp <- GET(url = base_url, query = query_params)
    
# Parse response
resp_xml <- content(resp)
```

### Extracting information

Now we have a response from the API, we need to extract the HTML for the page from it. It turns out the HTML is stored in the contents of the XML response.
Take a look, by using xml_text() to pull out the text from the XML response:

xml_text(resp_xml)
In this exercise, you'll read this text as HTML, then extract the relevant nodes to get the infobox and page title.

Instructions
Code from the previous exercise has already been run, so you have resp_xml available in your workspace.

Use read_html() to read the contents of the XML response (xml_text(resp_xml)) as HTML.
Use html_node() to extract the infobox element (having the class infobox) from page_html with a CSS selector.
Use html_node() to extract the page title element (having the class fn) from infobox_element with a CSS selector.
Extract the title text from page_name with html_text().

```{r}
# Load rvest
library(rvest)

# Read page contents as HTML
page_html <- read_html(xml_text(resp_xml))
page_html
# Extract infobox element
infobox_element <- html_node(page_html, css = ".infobox")
infobox_element
# Extract page name element from infobox
page_name <- html_node(page_html, css=".fn")
page_name
# Extract page name as text
page_title <- html_text(page_name)
page_title

```

### Normalising information

Now it's time to put together the information in a nice format. You've already seen you can use html_table() to parse the infobox into a data frame. But one piece of important information is missing from that table: who the information is about!

In this exercise, you'll parse the infobox in a data frame, and add a row for the full name of the subject.

Instructions
No need to repeat all the table parsing code from Chapter 4, we've already added it to your script.

Create a new data frame where key is the string "Full name" and value is our previously stored page_title.
Combine name_df with cleaned_table using rbind() and assign it to wiki_table2.
Print wiki_table2.

```{r}
# Your code from earlier exercises
wiki_table <- html_table(infobox_element)
colnames(wiki_table) <- c("key", "value")
cleaned_table <- subset(wiki_table, !key == "")
cleaned_table
# Create a dataframe for full name
name_df <- data.frame(key = "Full name", value = page_title)
name_df
# Combine name_df with cleaned_table
wiki_table2 <- rbind(name_df, cleaned_table)

# Print wiki_table
wiki_table2
```

### Reproducibility

Now you've figured out the process for requesting and parsing the infobox for the Hadley Wickham page, it's time to turn it into a function that does the same thing for anyone.

You've already done all the hard work! In the sample script we've just copied all your code from the previous three exercises, with only one change: we've wrapped it in the function definition syntax, and chosen the name get_infobox() for this function.

It doesn't quite work yet, the argument title isn't used inside the function. In this exercise you'll fix that, then test it out with some other personalities.

Instructions
Fix the function, by replacing the string "Hadley Wickham" with title, so that the title argument of the function will be used for the query.
Test get_infobox() with title = "Hadley Wickham".
Now, try getting the infobox for "Ross Ihaka".
Finally, try getting the infobox for "Grace Hopper"

```{r}
library(httr)
library(rvest)
library(xml2)

get_infobox <- function(title){
  base_url <- "https://en.wikipedia.org/w/api.php"
  
  # Change "Hadley Wickham" to title
  query_params <- list(action = "parse", 
    page = title, 
    format = "xml")
  
  resp <- GET(url = base_url, query = query_params)
  resp_xml <- content(resp)
  
  page_html <- read_html(xml_text(resp_xml))
  infobox_element <- html_node(x = page_html, css =".infobox")
  page_name <- html_node(x = infobox_element, css = ".fn")
  page_title <- html_text(page_name)
  
  wiki_table <- html_table(infobox_element)
  colnames(wiki_table) <- c("key", "value")
  cleaned_table <- subset(wiki_table, !wiki_table$key == "")
  name_df <- data.frame(key = "Full name", value = page_title)
  wiki_table <- rbind(name_df, cleaned_table)
  
  wiki_table
}

# Test get_infobox with "Hadley Wickham"
get_infobox(title = "Hadley Wickham")

# Try get_infobox with "Ross Ihaka"
get_infobox(title = "Ross Ihaka")

# Try get_infobox with "Grace Hopper"
get_infobox(title = "Grace Hopper")



```

